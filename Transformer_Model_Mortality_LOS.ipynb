{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Transformer Model -- Mortality, LOS","provenance":[{"file_id":"1MLb_JAqjtIVJlHktHaXSCP5jWFQqLDmM","timestamp":1589202658312},{"file_id":"1xCVo8S3Pz_x-4yleF_MkRWNGzw2SslTf","timestamp":1588019404982},{"file_id":"1WDTWvfZvIrcclqAs5KMqEroQ5rtaB3yJ","timestamp":1587625813876},{"file_id":"1Jxj3hQ5pSST0KmnFG2TKUslG-00eFMWc","timestamp":1583092405360},{"file_id":"19tSvuHLZ3fawJk4_72YhU3xtLJ3FOAWd","timestamp":1582605945703},{"file_id":"1AOw9OTa6m6jEC9RBsWyWPOBdVjBcSjQQ","timestamp":1582513757775}],"private_outputs":true,"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sBEzAm_FGp8v","colab_type":"code","colab":{}},"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import math, copy, time\n","from torch.autograd import Variable\n","from torch.utils import data\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import pickle\n","from torchtext import data, datasets\n","\n","from google.colab import auth\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cncAwaepFSd0","colab_type":"text"},"source":["# Read in data into relevant DF's, remove all but the first admission"]},{"cell_type":"code","metadata":{"id":"khpk-ADtGvmk","colab_type":"code","colab":{}},"source":["!wget -r -N -c -np --user kyleliu --ask-password https://physionet.org/files/picdb/1.0.0/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"363L7jzp6zz3","colab_type":"code","colab":{}},"source":["# Read Data into DF\n","\n","admissions = pd.read_csv('physionet.org/files/picdb/1.0.0/ADMISSIONS.csv.gz', compression='gzip')\n","chartevents = pd.read_csv('physionet.org/files/picdb/1.0.0/CHARTEVENTS.csv.gz', compression='gzip')\n","diagnoses_icd = pd.read_csv('physionet.org/files/picdb/1.0.0/DIAGNOSES_ICD.csv.gz', compression='gzip')\n","d_icd_diagnoses = pd.read_csv('physionet.org/files/picdb/1.0.0/D_ICD_DIAGNOSES.csv.gz', compression='gzip')\n","d_items = pd.read_csv('physionet.org/files/picdb/1.0.0/D_ITEMS.csv.gz', compression='gzip')\n","d_labitems = pd.read_csv('physionet.org/files/picdb/1.0.0/D_LABITEMS.csv.gz', compression='gzip')\n","emr_symptoms = pd.read_csv('physionet.org/files/picdb/1.0.0/EMR_SYMPTOMS.csv.gz', compression='gzip')\n","icu_stays = pd.read_csv('physionet.org/files/picdb/1.0.0/ICUSTAYS.csv.gz', compression='gzip')\n","input_events = pd.read_csv('physionet.org/files/picdb/1.0.0/INPUTEVENTS.csv.gz', compression='gzip')\n","lab_events = pd.read_csv('physionet.org/files/picdb/1.0.0/LABEVENTS.csv.gz', compression='gzip')\n","patients = pd.read_csv('physionet.org/files/picdb/1.0.0/PATIENTS.csv.gz', compression='gzip')\n","prescriptions = pd.read_csv('physionet.org/files/picdb/1.0.0/PRESCRIPTIONS.csv.gz', compression='gzip')\n","surgery_vital_signs = pd.read_csv('physionet.org/files/picdb/1.0.0/SURGERY_VITAL_SIGNS.csv.gz', compression='gzip')\n","microbiology_events = pd.read_csv('physionet.org/files/picdb/1.0.0/MICROBIOLOGYEVENTS.csv.gz', compression='gzip')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PdDz95lpKJ6R","colab_type":"code","colab":{}},"source":["# Easier to use: \n","\n","item_dict = dict() \n","for _, row in d_items.iterrows(): \n","  item_dict[row.ITEMID] = row.LABEL\n","\n","lab_item_dict = dict()\n","for _, row in d_labitems.iterrows(): \n","  lab_item_dict[row.ITEMID] = row.LABEL\n","\n","ICD_CN_TO_ICD = dict() \n","for _, row in d_icd_diagnoses.iterrows(): \n","  ICD_CN_TO_ICD[row.ICD10_CODE_CN] = row.ICD10_CODE \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L6oefTPmDrgy","colab_type":"text"},"source":["Here we include only the first admission of each patient."]},{"cell_type":"code","metadata":{"id":"vQI20hqJH2iK","colab_type":"code","colab":{}},"source":["# Clean: Include only the first admission\n","\n","admissions = admissions.sort_values(by = ['ADMITTIME'])\n","chartevents = chartevents.sort_values(by = ['CHARTTIME'])\n","lab_events = lab_events.sort_values(by = ['CHARTTIME'])\n","\n","admits_to_keep = []\n","seen_patients = set()\n","\n","for _, row in admissions.iterrows(): \n","  if row.SUBJECT_ID not in seen_patients: \n","    admits_to_keep.append(row.HADM_ID)\n","    seen_patients.add(row.SUBJECT_ID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gcz6Z57DMUSI","colab_type":"code","colab":{}},"source":["def remove_admits(df): \n","  return df[df['HADM_ID'].isin(admits_to_keep)]\n","\n","admissions = remove_admits(admissions)\n","chartevents = remove_admits(chartevents)\n","diagnoses_icd = remove_admits(diagnoses_icd)\n","emr_symptoms = remove_admits(emr_symptoms)\n","icu_stays = remove_admits(icu_stays)\n","input_events = remove_admits(input_events)\n","lab_events = remove_admits(lab_events)\n","prescriptions = remove_admits(prescriptions)\n","surgery_vital_signs = remove_admits(surgery_vital_signs)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"brSKZucgFqm9","colab_type":"text"},"source":["# Helper Functions for Date/Times. Allign by first admit time (hours)"]},{"cell_type":"code","metadata":{"id":"zCEP8hHyIA5s","colab_type":"code","colab":{}},"source":["from datetime import date, timedelta, time, datetime\n","\n","def to_datetime(x): \n","  li = x.split()\n","  my_date = li[0].split(\"-\")\n","  my_time = li[1].split(\":\")\n","\n","  ret = datetime(int(my_date[0]), int(my_date[1]), int(my_date[2]), int(my_time[0]), int(my_time[1]), int(my_time[2]))\n","  \n","  return ret\n","\n","age_at_admission = dict()  \n","birth_date = dict()\n","admit_date = dict() \n","for _, row in patients.iterrows(): \n","  birth_date[row.SUBJECT_ID] = to_datetime(row.DOB)\n","\n","for _, row in admissions.iterrows(): \n","  admit_date[row.SUBJECT_ID] = to_datetime(row.ADMITTIME)\n","  age_at_admission[row.SUBJECT_ID] = to_datetime(row.ADMITTIME) - birth_date[row.SUBJECT_ID]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1a-Fl9r0TTcp","colab_type":"code","colab":{}},"source":["# Time since admission (hours)\n","\n","def isFloat(string):\n","    try:\n","        float(string)\n","        return True\n","    except ValueError:\n","        return False\n","\n","def normalize_time(patient_id, x): \n","  if isFloat(x): \n","    return 100000\n","  delta = to_datetime(x) - admit_date[patient_id]\n","  return delta.total_seconds() / 3600.0 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0rL1DdpvVh5q","colab_type":"code","colab":{}},"source":["patient_set = [p for p in patients.SUBJECT_ID]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ss7GMc9FCcOE","colab_type":"code","colab":{}},"source":["chartevents['HOURS_IN'] = chartevents.apply(lambda row: normalize_time(row.SUBJECT_ID, row.CHARTTIME), axis=1)\n","lab_events['HOURS_IN'] = lab_events.apply(lambda row: normalize_time(row.SUBJECT_ID, row.CHARTTIME), axis=1)\n","surgery_vital_signs['HOURS_IN'] = surgery_vital_signs.apply(lambda row: normalize_time(row.SUBJECT_ID, row.MONITORTIME), axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ExCwUewmFzfA","colab_type":"text"},"source":["# Feature Specification"]},{"cell_type":"markdown","metadata":{"id":"Fqjkr1HzF3RQ","colab_type":"text"},"source":["Here we do our feature selection. Right now I have something very simple: most common chart, lab events, as well as age/gender (usually concatenated to the other two at each time step)"]},{"cell_type":"code","metadata":{"id":"8XlhXYs49KhC","colab_type":"code","colab":{}},"source":["import math \n","## Feature Set\n","\n","## Chart Features\n","chart_feats = [1001, 1002, 1003, 1004, 1006, 1007, 1008, 1009, 1010, 1011, 1012, 1013, 1014, 1015, 1016]\n","\n","# Surgery Vital Signs\n","surgery_feats = surgery_vital_signs['ITEMID'].value_counts().index.tolist() \n","\n","lab_feats = [5225, \n","             5097, \n","             5141, \n","             5129, \n","             5257, \n","             5132\n","             ]\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OSCoo8grlA9s","colab_type":"text"},"source":["We use these to index into the tensors that follow (i.e. chart_X[patient_index_of[subject_id]] is what you want, not chart_X[subject_id]. Similar for item_id's"]},{"cell_type":"code","metadata":{"id":"YObWSqyhOK7B","colab_type":"code","colab":{}},"source":["# More Helper Dicts\n","chart_index_of = dict() \n","for i in range(len(chart_feats)): \n","  chart_index_of[chart_feats[i]] = i\n","  \n","lab_index_of = dict() \n","for i in range(len(lab_feats)): \n","  lab_index_of[lab_feats[i]] = i\n","\n","surgery_index_of = dict() \n","for i in range(len(surgery_feats)): \n","  surgery_index_of[surgery_feats[i]] = i\n","\n","print(chart_index_of)\n","print(lab_index_of)\n","print(surgery_index_of)\n","\n","patient_index_of = dict() \n","cc = 0\n","for p in patient_set: \n","  patient_index_of[p] = cc \n","  cc += 1\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q0ijOJ0O8Y7Z","colab_type":"code","colab":{}},"source":["GAP_TIME          = 6  # In hours\n","WINDOW_SIZE       = 24 # Data collection window: In hours\n","# Label has to be first satisfied after GAP_TIME + WINDOW_SIZE\n","\n","# Generate per-hour aggregates (w/in the window).\n","\n","# For each patient, generate a time series of\n","\n","# Remove negative chart times\n","subjects_to_remove = set() \n","\n","for _, row in admissions.iterrows(): \n","  if normalize_time(row.SUBJECT_ID, row.DISCHTIME) < (GAP_TIME + WINDOW_SIZE): \n","    subjects_to_remove.add(row.SUBJECT_ID)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kkRov0eLbCnr","colab_type":"code","colab":{}},"source":["# Set up X, Y \n","\n","# Set up labels\n","\n","patient_set = list(patient_set)\n","\n","mort_icu = dict() \n","for _, row in patients.iterrows(): \n","  if row.SUBJECT_ID in patient_set: \n","    mort_icu[row.SUBJECT_ID] = row.EXPIRE_FLAG \n","\n","gender_one_hot = np.zeros((len(patient_set), 2))\n","age_vec = np.zeros((len(patient_set), 1))\n","for _, row in patients.iterrows(): \n","  if row.SUBJECT_ID in patient_set: \n","    age_vec[patient_index_of[row.SUBJECT_ID]][0] = (age_at_admission[row.SUBJECT_ID].total_seconds() / 3600.0)\n","    if row.GENDER == 'M': \n","      gender_one_hot[patient_index_of[row.SUBJECT_ID]][0] = 1\n","    else: \n","      gender_one_hot[patient_index_of[row.SUBJECT_ID]][1] = 1\n","\n","static_vec = np.concatenate((gender_one_hot, age_vec), axis = 1)\n","# [num_patients, 3]\n","\n","# time_vec [num_patients, window_size, num_lab_features + num_chart_features + num_vital_features]\n","\n","# concatenate this with static_vec [num_patients, 3]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfv1GuGRGJre","colab_type":"text"},"source":["# Setting Up Data for Aggregated Stuff"]},{"cell_type":"markdown","metadata":{"id":"alE7SPf4mYwZ","colab_type":"text"},"source":["A lot of these setups are task specific. Like I said earlier, the inputs X are the same to each model, but you should flatten them for LR/RF.\n","\n","The labels need to be different. See how I've defined Y in each of the models.\n","For LR/RF, we can leave Y as-is. For LSTM, we try to predict Y at each time point, so we should expand Y (by stacking WINDOW_SIZE copies of it).\n","\n","Also, we need to protect against label leakage uniquely for each task. For instance, SEPTIC[i] records the first time patient i had the sepsis label (in hours). If SEPTIC[i] < GAP_TIME + WINDOW_SIZE, we have risk of label leakage, so we need to remove it. Append the subject id to my_subjects_to_remove.\n","\n","Once done, get a boolean mask from get_mask(my_subjects_to_remove), and index into your covariates / labels accordingly (return covars[mask, ...], labels[mask, ...])"]},{"cell_type":"code","metadata":{"id":"szGv-3ppjq9L","colab_type":"code","colab":{}},"source":["def get_mask(removed_subjects): \n","\n","  mask = [True for p in patient_set]\n","  for p in removed_subjects:\n","    mask[patient_index_of[p]] = False\n","\n","  return mask \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"y0LyoBbValp-","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_curve\n","def plot_roc(title, labels, probs): \n","  fpr, tpr, thresholds = roc_curve(labels, probs) \n","  plt.figure()\n","  plt.plot(fpr, tpr, label=title)\n","  plt.plot([0, 1], [0, 1],'r--')\n","  plt.xlim([0.0, 1.0])\n","  plt.ylim([0.0, 1.05])\n","  plt.xlabel('1 - Specificity')\n","  plt.ylabel('Sensitivity')\n","  plt.title('ROC')\n","  plt.legend(loc=\"lower right\")\n","  plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10cnz5AjMg-f","colab_type":"text"},"source":["Note we need to further preprocess this data (zero mean, unit variance, PCA, etc..)\n"]},{"cell_type":"markdown","metadata":{"id":"DQKlxdFf33Zz","colab_type":"text"},"source":["Let's also do some upsampling to try to deal with class imbalance."]},{"cell_type":"markdown","metadata":{"id":"625MmHclGiE1","colab_type":"text"},"source":["# Transformer"]},{"cell_type":"markdown","metadata":{"id":"CxNTdUtk6c7K","colab_type":"text"},"source":["Transformer Architecture. Motivated by https://arxiv.org/pdf/1711.03905.pdf page 3. We use a different positional embedding structure, and pass the outputs of the Encoder directly into our output layer."]},{"cell_type":"markdown","metadata":{"id":"Z3TJ1BgEM65J","colab_type":"text"},"source":["## Wrapper Classes"]},{"cell_type":"markdown","metadata":{"id":"edqR3Bln6uMj","colab_type":"text"},"source":["Here, we create our wrapper class. Takes in src, src_times, src_mask"]},{"cell_type":"code","metadata":{"id":"GAPedMPk6cQD","colab_type":"code","colab":{}},"source":["class Transformer(nn.Module):\n","    def __init__(self, encoder, decoder, pos_embed, embedding, generator, type='Interpolated'):\n","        super(Transformer, self).__init__()\n","        # decoder is some modified crap.\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.pos_embed = pos_embed\n","        self.embedding = embedding\n","        self.generator = generator\n","        self.type = type\n","        \n","    def forward(self, src, src_times, src_mask):\n","        # Inputs: \n","        # Src: (B x T x D_input)\n","        # Src_times: (B x T)\n","        # Src_mask:  (B x 1 x D)\n","        encoder_output = self.encode(src, src_times, src_mask)\n","        if self.type == 'Interpolated': \n","          return self.decode(encoder_output, src_times, src_mask)\n","        else: \n","          return encoder_output[:, 0, :]\n","\n","    \n","    def encode(self, src, src_times, src_mask):\n","        inp = self.pos_embed(self.embedding(src), src_times)\n","        return self.encoder(inp, src_mask)\n","    \n","    def decode(self, memory, src_times, src_mask):\n","        return self.decoder(memory, src_times, src_mask)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"v3-JDKxpG3o6","colab_type":"code","colab":{}},"source":["class Generator(nn.Module):\n","    \"Standard generation step. (Not described in the paper.)\"\n","    def __init__(self, d_model, M, vocab, type='Interpolated'):\n","      super(Generator, self).__init__()\n","      self.proj = nn.Linear(d_model, vocab)\n","      self.proj2 = nn.Linear(d_model*M, vocab)\n","      self.type = type\n","    def forward(self, x):\n","      if self.type == 'Interpolated': \n","        return self.proj2(x)\n","      else: \n","        return self.proj(x)\n","      "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AXyx4o70619Z","colab_type":"text"},"source":["Once we get the logits out of EncoderDecoder, we use this to get our output."]},{"cell_type":"markdown","metadata":{"id":"VUwYcPdf6-tT","colab_type":"text"},"source":["Encoder  Stacks. The original paper uses N = 6 stacks for encoder and decoder. Let's just use that for now.\n","\n"]},{"cell_type":"code","metadata":{"id":"6gm7ws7uYbHZ","colab_type":"code","colab":{}},"source":["def clones(module, N):\n","    \"Produce N identical layers.\"\n","    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uP7izP6yLikq","colab_type":"text"},"source":["## Layer Normalization"]},{"cell_type":"markdown","metadata":{"id":"kT5CEWGL7VAn","colab_type":"text"},"source":["After multihead attention and after the feed forward network, we normalize layers."]},{"cell_type":"code","metadata":{"id":"LZRCbD287QVX","colab_type":"code","colab":{}},"source":["class LayerNorm(nn.Module):\n","    \"Construct a layernorm module (See citation for details).\"\n","    def __init__(self, features, eps=1e-6):\n","        super(LayerNorm, self).__init__()\n","        self.a_2 = nn.Parameter(torch.ones(features))\n","        self.b_2 = nn.Parameter(torch.zeros(features))\n","        self.eps = eps\n","\n","    def forward(self, x):\n","        mean = x.mean(-1, keepdim=True)\n","        std = x.std(-1, keepdim=True)\n","        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E1sya3R77ihZ","colab_type":"text"},"source":["The output of each sublayer S with input x is LayerNorm(x + S(x)). We should definitely apply dropout here to both the input x and the output of LayerNorm as well. All sublayers produce outputs of d_model = 512."]},{"cell_type":"markdown","metadata":{"id":"WRl0dmr_LqFe","colab_type":"text"},"source":["## Encoder Specification"]},{"cell_type":"code","metadata":{"id":"8ZhglrWG7Ln6","colab_type":"code","colab":{}},"source":["class Encoder(nn.Module):\n","    \"Core encoder is a stack of N layers\"\n","    def __init__(self, layer, N):\n","        super(Encoder, self).__init__()\n","        self.layers = clones(layer, N)\n","        self.norm = LayerNorm(layer.size)\n","        \n","    def forward(self, x, mask):\n","        \"Pass the input (and mask) through each layer in turn.\"\n","        for layer in self.layers:\n","            x = layer(x, mask)\n","        return self.norm(x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z0mlUton79F_","colab_type":"text"},"source":["Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed- forward network."]},{"cell_type":"code","metadata":{"id":"ek-O4L4E76A1","colab_type":"code","colab":{}},"source":["class EncoderLayer(nn.Module):\n","    \"Encoder is made up of self-attn and feed forward (defined below)\"\n","    def __init__(self, size, self_attn, feed_forward, dropout):\n","        super(EncoderLayer, self).__init__()\n","        self.self_attn = self_attn\n","        self.feed_forward = feed_forward\n","        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n","        self.size = size\n","\n","    def forward(self, x, mask):\n","        \"Follow Figure 1 (left) for connections.\"\n","        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n","        return self.sublayer[1](x, self.feed_forward)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mHwBUwYG7f3t","colab_type":"code","colab":{}},"source":["class SublayerConnection(nn.Module):\n","    \"\"\"\n","    A residual connection followed by a layer norm.\n","    Note for code simplicity the norm is first as opposed to last.\n","    \"\"\"\n","    def __init__(self, size, dropout=0.0):\n","        super(SublayerConnection, self).__init__()\n","        self.norm = LayerNorm(size)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x, sublayer):\n","        \"Apply residual connection to any sublayer with the same size.\"\n","        return x + self.dropout(sublayer(self.norm(x)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0VTpQGMVLwth","colab_type":"text"},"source":["## Decoder (Returns Dense Interpolation)"]},{"cell_type":"markdown","metadata":{"id":"magY6sIaQOvu","colab_type":"text"},"source":["Take the logits out of Encoder, pass into this. We use a modification of dense interpolation for continous values. Let's see how this works. This returns a dense interpolation of dimension B x (d_model*M)"]},{"cell_type":"code","metadata":{"id":"vlR8bEcgGDqn","colab_type":"code","colab":{}},"source":["#### Might not need to unsqueeze depending on src_mask\n","\n","\n","class Decoder(nn.Module):\n","    def __init__(self, d_model, max_time, M):\n","        super(Decoder, self).__init__()\n","        self.M = M\n","        self.max_time = max_time\n","        \n","    def forward(self, x, times, src_mask):\n","      ### Returns a Logit\n","      ### x.size = (B, T, D)\n","      ### times.size = (B, T)\n","      ### src_mask.size = (B, 1, T) \n","      x = x.masked_fill(src_mask.transpose(-2, -1).expand_as(x) == False, 0)\n","      x = self.dense_interpolate(x, times, src_mask)\n","      return x\n","\n","    def dense_interpolate(self, x, times, src_mask): \n","      B = x.size(0)\n","      T = x.size(1) \n","\n","      s = self.M * times / self.max_time # B x T \n","      m = (torch.arange(self.M) + 1).unsqueeze(0).expand((B, self.M)) # B x M \n","      w = torch.abs(s.unsqueeze(2).expand((B, T, self.M)) # B x T x M\n","                    - m.unsqueeze(1).expand((B, T, self.M))) / self.M  \n","      w = (1.0 - w) * (1.0 - w)\n","      \n","      w.masked_fill_(src_mask.transpose(1, 2).expand(B, T, self.M) == 0, 0)\n","\n","      u = torch.bmm(x.transpose(1, 2), w) # B x D x M \n","      u = u.reshape(u.size(0), u.size(1) * u.size(2))\n","      return u\n","\n","          \n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LMaLsVp8L6dO","colab_type":"text"},"source":["## Attention Mechanism"]},{"cell_type":"markdown","metadata":{"id":"MZ11omOyPp5H","colab_type":"text"},"source":["Attention Mechanism -- Concise via query, key, value matrices -- Luong Attention. Divide by sqrt(dimension) so that the result still has zero mean, unit variance."]},{"cell_type":"code","metadata":{"id":"7Vy0rT-UOt4C","colab_type":"code","colab":{}},"source":["def attention(query, key, value, mask=None, dropout=None):\n","    \"Compute 'Scaled Dot Product Attention'\"\n","    d_k = query.size(-1)\n","    scores = torch.matmul(query, key.transpose(-2, -1)) \\\n","             / math.sqrt(d_k)\n","    if mask is not None:\n","        scores = scores.masked_fill(mask == 0, -1e9)\n","    p_attn = F.softmax(scores, dim = -1)\n","    if dropout is not None:\n","        p_attn = dropout(p_attn)\n","    return torch.matmul(p_attn, value), p_attn"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MscbWXQySgoH","colab_type":"text"},"source":["Multi-headed attention. Each has dimension d_k = d_v = d_model / H. We use 8-head attention -- 64.\n"]},{"cell_type":"code","metadata":{"id":"UonTeqj7TpXt","colab_type":"code","colab":{}},"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, h, d_model, dropout=0.1):\n","        \"Take in model size and number of heads.\"\n","        super(MultiHeadedAttention, self).__init__()\n","        assert d_model % h == 0\n","        # We assume d_v always equals d_k\n","        self.d_k = d_model // h\n","        self.h = h\n","        self.linears = clones(nn.Linear(d_model, d_model), 4)\n","        self.attn = None\n","        self.dropout = nn.Dropout(p=dropout)\n","        \n","    def forward(self, query, key, value, mask=None):\n","        \"Implements Figure 2\"\n","        if mask is not None:\n","            # Same mask applied to all h heads.\n","            mask = mask.unsqueeze(1)\n","        nbatches = query.size(0)\n","        \n","        # 1) Do all the linear projections in batch from d_model => h x d_k \n","        query, key, value = \\\n","            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n","             for l, x in zip(self.linears, (query, key, value))]\n","        \n","        # 2) Apply attention on all the projected vectors in batch. \n","        x, self.attn = attention(query, key, value, mask=mask, \n","                                 dropout=self.dropout)\n","        \n","        # 3) \"Concat\" using a view and apply a final linear. \n","        x = x.transpose(1, 2).contiguous() \\\n","             .view(nbatches, -1, self.h * self.d_k)\n","        return self.linears[-1](x)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keMSVtydL_Um","colab_type":"text"},"source":["## Pointwise FFN"]},{"cell_type":"markdown","metadata":{"id":"sndHCE7-Tupg","colab_type":"text"},"source":["In addition to attention, we apply a feed forward network in each encoder/decoder layer. Hidden layer has d_ff = 256."]},{"cell_type":"code","metadata":{"id":"PagyuXtRT5rP","colab_type":"code","colab":{}},"source":["class PositionwiseFeedForward(nn.Module):\n","    \"Implements FFN equation.\"\n","    def __init__(self, d_model, d_ff, dropout=0.1):\n","        super(PositionwiseFeedForward, self).__init__()\n","        self.w_1 = nn.Linear(d_model, d_ff)\n","        self.w_2 = nn.Linear(d_ff, d_model)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bA0q9bC3MCo6","colab_type":"text"},"source":["## Feature + Position Embeddings"]},{"cell_type":"markdown","metadata":{"id":"83Axb6bLzNc-","colab_type":"text"},"source":["Embedding Layer. We have a feature vector of length d_input. Concatenate all numerical features (centered / scaled) with one-hots for all pos/neg tests. Include age. Time is not included！We pass in times separately to the positional encoding.\n"]},{"cell_type":"code","metadata":{"id":"AkJiFEWL--HQ","colab_type":"code","colab":{}},"source":["class Embeddings(nn.Module): \n","  def __init__(self, d_input, d_hidden, d_model, dropout=0.1): \n","    super(Embeddings, self).__init__() \n","    self.d_input = d_input\n","    self.d_hidden = d_hidden\n","    self.d_model = d_model \n","\n","    self.w_1 = nn.Linear(d_input, d_hidden)\n","    self.w_2 = nn.Linear(d_hidden, d_model)\n","    self.dropout = nn.Dropout(dropout)\n","  \n","  def forward(self, x): \n","    return self.w_2(self.dropout(F.relu(self.w_1(x))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d2VZpDOyV5k3","colab_type":"text"},"source":["We still need to use positional encodings. Let's use this. https://openreview.net/attachment?id=SkeJMCVFDS&name=original_pdf \n","\n","PE_(t, 2i) = sin(t / T^(2i / d))\n","PE_(t, 2i+1) = cos(t / T^(2i / d))"]},{"cell_type":"code","metadata":{"id":"vULKEF9yV4vU","colab_type":"code","colab":{}},"source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_time=1000):\n","        super(PositionalEncoding, self).__init__()\n","        self.d_model = d_model\n","        self.max_time = max_time\n","        self.dropout = nn.Dropout(p=dropout)\n","        self.div_term = torch.exp(torch.arange(0., d_model, 2) *\n","                             -(math.log(max_time) / d_model))\n","        \n","    def forward(self, x, times):\n","      # x: (batch, seq_len, dim)\n","      # times: (batch, seq_len)\n","        pe = torch.zeros(x.size(0), x.size(1), self.d_model)\n","        position = times.unsqueeze(2)\n","\n","        pe[:, :, 0::2] = torch.sin(position * self.div_term)\n","        pe[:, :, 1::2] = torch.cos(position * self.div_term)\n","\n","        x = x + Variable(pe)\n","\n","        return self.dropout(x)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2CbKHCgWjm0","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(15, 5))\n","pe = PositionalEncoding(20, 0)\n","x = torch.zeros(5, 100, 20)\n","t = torch.arange(0., 100, 1).unsqueeze(0).expand(x.shape[0:2])\n","y = pe.forward(Variable(x), t)\n","plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n","plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"R0qT9u2DMJYS","colab_type":"text"},"source":["## Instancing a Model"]},{"cell_type":"markdown","metadata":{"id":"mroKALFNlPPP","colab_type":"text"},"source":["Now let's make our model."]},{"cell_type":"code","metadata":{"id":"wMUkyQSclQUd","colab_type":"code","colab":{}},"source":["def make_model(d_input, d_target, N=6, M=12, d_model=128, d_hidden=256, d_ff=512, h=8, max_observation_time=24.0, max_encoding_time=1000.0, dropout=0.1, type='Interpolated'):\n","    \"Helper: Construct a model from hyperparameters.\"\n","    c = copy.deepcopy\n","    attn = MultiHeadedAttention(h, d_model)\n","    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n","    position = PositionalEncoding(d_model, dropout, max_time = max_encoding_time)\n","\n","    model = Transformer(\n","        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n","        Decoder(d_model, max_time=max_observation_time, M=M),\n","        c(position), Embeddings(d_input, d_hidden, d_model),\n","        Generator(d_model, M, d_target, type), \n","        type)\n","\n","    \n","    # This was important from their code. \n","    # Initialize parameters with Glorot / fan_avg.\n","    for p in model.parameters():\n","        if p.dim() > 1:\n","            nn.init.xavier_uniform(p)\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m9negvPxMOXt","colab_type":"text"},"source":["## Training Details"]},{"cell_type":"markdown","metadata":{"id":"MHiK469Yb5DK","colab_type":"text"},"source":["Now let's define our training regime."]},{"cell_type":"markdown","metadata":{"id":"O99P0NyPlsVm","colab_type":"text"},"source":["Optimizer Directly from srush (cite him)."]},{"cell_type":"code","metadata":{"id":"nTx83gBPlq9F","colab_type":"code","colab":{}},"source":["class NoamOpt:\n","    \"Optim wrapper that implements rate.\"\n","    def __init__(self, model_size, factor, warmup, optimizer):\n","        self.optimizer = optimizer\n","        self._step = 0\n","        self.warmup = warmup\n","        self.factor = factor\n","        self.model_size = model_size\n","        self._rate = 0\n","        \n","    def step(self):\n","        \"Update parameters and rate\"\n","        self._step += 1\n","        rate = self.rate()\n","        for p in self.optimizer.param_groups:\n","            p['lr'] = rate\n","        self._rate = rate\n","        self.optimizer.step()\n","        \n","    def rate(self, step = None):\n","        \"Implement `lrate` above\"\n","        if step is None:\n","            step = self._step\n","        return self.factor * \\\n","            (self.model_size ** (-0.5) *\n","            min(step ** (-0.5), step * self.warmup ** (-1.5)))\n","        \n","def get_std_opt(model):\n","    return NoamOpt(model.pos_embed.d_model, 2, 4000,\n","            torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"M3IQZao5l0pl","colab_type":"code","colab":{}},"source":["# Three settings of the lrate hyperparameters.\n","opts = [NoamOpt(512, 1, 4000, None), \n","        NoamOpt(512, 1, 8000, None),\n","        NoamOpt(256, 1, 4000, None)]\n","plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n","plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])\n","None"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wxb0crtFl-67","colab_type":"text"},"source":["Let's use label smoothing as well."]},{"cell_type":"markdown","metadata":{"id":"DFV8EcNRDf-Y","colab_type":"text"},"source":["### Training Loops"]},{"cell_type":"code","metadata":{"id":"6sah6vpMDVf6","colab_type":"code","colab":{}},"source":["import time\n","\n","def timeSince(since):\n","    now = time.time()\n","    s = now - since\n","    m = math.floor(s / 60)\n","    s -= m * 60\n","    return '%dm %ds' % (m, s)\n","\n","def train_epoch(train_iter, model, criterion, opt, transpose=False):\n","    i = 0\n","    total = 0.0\n","\n","    start = time.time()\n","\n","\n","    model.train()\n","    for src, src_times, src_mask, trg in train_iter:\n","\n","        i += 1\n","\n","        model.zero_grad()\n","        src_mask = src_mask.unsqueeze(-2)\n","        out = model.forward(src, src_times, src_mask)\n","        gen = model.generator(out)\n","        \n","        loss = criterion(gen.squeeze(-1), trg) \n","        loss.backward()\n","        model_opt.step()\n","\n","        total += loss.item()\n","\n","        if i % 20 == 19:\n","            print(i, \" Batches, Average Loss: \", total/20, \" Opt Rate: \", model_opt._rate, \" Time: \", timeSince(start))\n","            total = 0.0\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"spmALSxkDXRd","colab_type":"code","colab":{}},"source":["def valid_epoch(valid_iter, model, criterion, transpose=False):\n","    \n","    i = 0\n","    probs = []\n","    preds = [] \n","    trgs = []\n","    model.eval()\n","    for src, src_times, src_mask, trg in valid_iter:\n","        src_mask = src_mask.unsqueeze(-2)\n","        out = model.forward(src, src_times, src_mask)\n","        gen = model.generator(Variable(out.data, requires_grad=False))\n","        prob = torch.sigmoid(gen).data\n","        probs.extend(prob)\n","        preds.extend((prob >= 0.5).float())\n","        trgs.extend(trg)\n","        i += 1\n","      \n","    \n","    return probs, preds, trgs\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bdiYnGjVDlMr","colab_type":"text"},"source":["## Batch Stuff"]},{"cell_type":"markdown","metadata":{"id":"95eRz17efZ5o","colab_type":"text"},"source":["# Set Up Time Series (Full Resolution)"]},{"cell_type":"markdown","metadata":{"id":"X2TVqFVNfyc9","colab_type":"text"},"source":["Here let's put the age and gender in at each time point. Categoricals are one-hotted, continuous are centered/scaled."]},{"cell_type":"code","metadata":{"id":"fUpEcOJ6y7oX","colab_type":"code","colab":{}},"source":["# Lol\n","def is_number(s):\n","    try:\n","        float(s)\n","        return True\n","    except ValueError:\n","        return False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DRrx4kuEPX8y","colab_type":"code","colab":{}},"source":["# First let's get the set of timestamps.  \n","MAX_TIMESTAMPS = 150\n","LEN_FEATS = len(chart_feats)+len(surgery_feats)+len(lab_feats)\n","PAD = '9999-01-01 01:01:01'\n","PAD_TIME = 10000000\n","# Max # of timestamps for a guy is ~150 for window size of 24 hours.\n","cnt = 0\n","\n","keep_tmp = np.unique(np.concatenate((lab_events.SUBJECT_ID, chartevents.SUBJECT_ID, surgery_vital_signs.SUBJECT_ID)))\n","rem_tmp = np.concatenate((lab_events[lab_events['HOURS_IN'] < 0].SUBJECT_ID, chartevents[chartevents['HOURS_IN'] < 0].SUBJECT_ID, surgery_vital_signs[surgery_vital_signs['HOURS_IN'] < 0].SUBJECT_ID))\n","\n","X_feats = np.zeros((len(patient_set), MAX_TIMESTAMPS, LEN_FEATS))\n","X_present = np.zeros((len(patient_set), MAX_TIMESTAMPS, LEN_FEATS)) == 1\n","X_times = np.zeros((len(patient_set), MAX_TIMESTAMPS))\n","X_masks = np.zeros((len(patient_set), MAX_TIMESTAMPS)) == 1\n","\n","time_lists = [list() for p in patient_set]\n","\n","for p in patient_set: \n","  if p not in keep_tmp: \n","    subjects_to_remove.add(p)  \n","  elif p in rem_tmp: \n","    subjects_to_remove.add(p)\n","\n","# Compute Times \n","for _, row in chartevents[chartevents['HOURS_IN'] < WINDOW_SIZE][chartevents['ITEMID'].isin(chart_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE): \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_lists[my_idx].append(row.CHARTTIME) \n","\n","for _, row in lab_events[lab_events['HOURS_IN'] < WINDOW_SIZE][lab_events['ITEMID'].isin(lab_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE): \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_lists[my_idx].append(row.CHARTTIME) \n","\n","for _, row in surgery_vital_signs[surgery_vital_signs['HOURS_IN'] < WINDOW_SIZE][surgery_vital_signs['ITEMID'].isin(surgery_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE):  \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_lists[my_idx].append(row.MONITORTIME) \n","\n","for i in range(len(patient_set)):\n","  time_lists[i] = sorted(list(set(time_lists[i])))\n","  \n","lens = [len(time_lists[i]) for i in range(len(patient_set))]\n","print(max(lens))\n","\n","for _, row in chartevents[chartevents['HOURS_IN'] < WINDOW_SIZE][chartevents['ITEMID'].isin(chart_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE):  \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_idx = time_lists[my_idx].index(row.CHARTTIME) + 1\n","    X_feats[my_idx][time_idx][chart_index_of[row.ITEMID]] = row.VALUE\n","    X_present[my_idx][time_idx][chart_index_of[row.ITEMID]] = True\n","    X_masks[my_idx][time_idx] = True\n","\n","for _, row in lab_events[lab_events['HOURS_IN'] < WINDOW_SIZE][lab_events['ITEMID'].isin(lab_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE):  \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_idx = time_lists[my_idx].index(row.CHARTTIME) + 1\n","    X_feats[my_idx][time_idx][len(chart_feats) + lab_index_of[row.ITEMID]] = row.VALUE\n","    X_present[my_idx][time_idx][len(chart_feats) + lab_index_of[row.ITEMID]] = True\n","    X_masks[my_idx][time_idx] = True\n","\n","for _, row in surgery_vital_signs[surgery_vital_signs['HOURS_IN'] < WINDOW_SIZE][surgery_vital_signs['ITEMID'].isin(surgery_feats)].iterrows(): \n","  if row.SUBJECT_ID in subjects_to_remove: \n","    continue \n","  elif is_number(row.VALUE):  \n","    my_idx = patient_index_of[row.SUBJECT_ID]\n","    time_idx = time_lists[my_idx].index(row.MONITORTIME) + 1\n","    X_feats[my_idx][time_idx][len(chart_feats) + len(lab_feats) + surgery_index_of[row.ITEMID]] = row.VALUE\n","    X_present[my_idx][time_idx][len(chart_feats) + len(lab_feats) + surgery_index_of[row.ITEMID]] = True\n","    X_masks[my_idx][time_idx] = True\n","\n","for i in range(len(patient_set)):\n","  for j in range(len(time_lists[i])): \n","    time_lists[i][j] = normalize_time(patient_set[i], time_lists[i][j])\n","  time_lists[i].append(-1) # [CLS]\n","  time_lists[i] = sorted(list(set(time_lists[i])))\n","  for j in range(MAX_TIMESTAMPS): \n","    if j >= len(time_lists[i]): \n","      X_times[i][j] = PAD_TIME\n","    else: \n","      if j == 0: \n","        X_times[i][j] = -1\n","      else: \n","        X_times[i][j] = normalize_time(patient_set[i], time_lists[i][j])\n","  X_masks[i][0] = True \n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlEdnyX-RHlU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JfC7xz1b_fdv","colab_type":"code","colab":{}},"source":["# Add in static_vec to non [CLS] states.\n","print(X_feats.shape)\n","static_add = np.expand_dims(static_vec, axis = 1)\n","static_add = np.tile(static_add, (1, X_feats.shape[1], 1))\n","static_msk = np.zeros(static_add.shape) == 0\n","\n","for i in range(len(patient_set)): \n","  for j in range(MAX_TIMESTAMPS): \n","    if X_times[i][j] == PAD_TIME: \n","      static_msk[i][j][:] = [False, False, False]\n","      static_add[i][j][:] = [0, 0, 0]\n","    elif j == 0: \n","      static_msk[i][j][:] = [False, False, False]\n","      static_add[i][j][:] = [0, 0, 0]\n","\n","\n","X_feats = np.concatenate((X_feats, static_add), axis=2)\n","X_present = np.concatenate((X_present, static_msk), axis=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RF3jbz8g9nd2","colab_type":"code","colab":{}},"source":["print(X_feats[0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kIA-FltKTA8_","colab_type":"code","colab":{}},"source":["# Add in [CLS] token.\n","new_row = np.zeros((1, MAX_TIMESTAMPS, 1))\n","new_row[0][0][0] = 1\n","new_row = np.tile(new_row, (X_feats.shape[0], 1, 1))\n","\n","X_feats = np.concatenate((new_row, X_feats), axis=2)\n","X_present = np.concatenate((new_row == 1, X_present), axis=2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_lxNvR1n7oPR","colab_type":"code","colab":{}},"source":["print(new_row.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"arC0wqXcU0zh","colab_type":"code","colab":{}},"source":["print(X_feats.shape)\n","print(X_present.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uT-_WkSaTcAV","colab_type":"code","colab":{}},"source":["LEN_FEATS += 4"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wipb7TAHfzyZ","colab_type":"text"},"source":["Generate Labels "]},{"cell_type":"code","metadata":{"id":"V33O_auIa1Jb","colab_type":"code","colab":{}},"source":["good_mask = get_mask(subjects_to_remove)\n","patients_to_include = np.array(patient_set)[good_mask]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6dfnag6jfjXF","colab_type":"code","colab":{}},"source":["Y_mort = np.zeros(len(patient_set)) \n","for i in range(len(patient_set)): \n","  if mort_icu[patient_set[i]] == 1:\n","    Y_mort[i] = 1\n","  else: \n","    Y_mort[i] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QWES2dLMeFrP","colab_type":"code","colab":{}},"source":["Y_3d = np.zeros(len(patient_set)) \n","Y_7d = np.zeros(len(patient_set))\n","for i in range(len(patient_set)): \n","  disch = admissions[admissions['SUBJECT_ID'] == patient_set[i]].DISCHTIME.item()\n","  if normalize_time(patient_set[i], disch) > (3 * 24): \n","    Y_3d[i] = 1\n","  else: \n","    Y_3d[i] = 0\n","  if normalize_time(patient_set[i], disch) > (7 * 24): \n","    Y_7d[i] = 1\n","  else: \n","    Y_7d[i] = 0"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jgoFBBdRiN5K","colab_type":"text"},"source":["Alright now here the stuff is set up. feats, times, masks need to be passed in to the transformer."]},{"cell_type":"code","metadata":{"id":"dB2GsiQS3vft","colab_type":"code","colab":{}},"source":["# X_feats = np.zeros((len(patient_set), MAX_TIMESTAMPS, LEN_FEATS))\n","# X_present = np.zeros((len(patient_set), MAX_TIMESTAMPS, LEN_FEATS)) == 1\n","# X_times = np.zeros((len(patient_set), MAX_TIMESTAMPS))\n","# X_masks = np.zeros((len(patient_set), MAX_TIMESTAMPS)) == 1"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2e_EUrIXm1i_","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","\n","train_patients, test_patients, _, _ = train_test_split([i for i in patients_to_include], Y_mort[good_mask], test_size=0.2, random_state=1)\n","\n","train_indices = [patient_index_of[i] for i in train_patients]\n","test_indices = [patient_index_of[i] for i in test_patients]\n","\n","### Let's manually normalize. \n","\n","global_means = np.zeros(LEN_FEATS)\n","global_std = np.zeros(LEN_FEATS) \n","\n","for i in range(1, LEN_FEATS): \n","  sum_here = np.sum(X_feats[:, :, i])\n","  num_here = np.sum(X_present[:, :, i])\n","\n","  global_means[i] = sum_here / num_here \n","\n","  square_sum = np.sum(X_feats[:, :, i] * X_feats[:, :, i])\n","  square_sum += (-2 * global_means[i] * sum_here)\n","  square_sum += global_means[i] * global_means[i] * num_here \n","\n","  global_std[i] = np.sqrt(square_sum / num_here)\n","\n","  X_feats[:, :, i] = (X_feats[:, :, i] - global_means[i]) / global_std[i]\n","  X_feats[:, :, i] = X_feats[:, :, i] * X_present[:, :, i]\n","\n","\n","train_mask = [False for p in patient_set] \n","test_mask = [True for p in patient_set] \n","for i in train_indices:\n","  train_mask[i] = True \n","  test_mask[i] = False \n","\n","X_train_feats = X_feats[train_mask, ...]\n","X_train_present = X_present[train_mask, ...]\n","X_train_times = X_times[train_mask, ...]\n","X_train_masks = X_masks[train_mask, ...]\n","Y_train_mort = Y_mort[train_mask, ...]\n","\n","X_test_feats = X_feats[test_mask, ...]\n","X_test_present = X_present[test_mask, ...]\n","X_test_times = X_times[test_mask, ...]\n","X_test_masks = X_masks[test_mask, ...]\n","Y_test_mort = Y_mort[test_mask, ...]\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d24Lc3YZfB-e","colab_type":"code","colab":{}},"source":["Y_train_3d = Y_3d[train_mask, ...]\n","Y_train_7d = Y_7d[train_mask, ...]\n","Y_test_3d = Y_3d[test_mask, ...]\n","Y_test_7d = Y_7d[test_mask, ...]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdZ7rSv2-Hnn","colab_type":"code","colab":{}},"source":["print(X_train_present[0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AVVdb9fVkaHu","colab_type":"code","colab":{}},"source":["print(X_train_present[0][0])\n","print(X_train_present[0][0].shape)\n","print(X_train_times[0][0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tlR3yBb5HUn_","colab_type":"text"},"source":["Dimensions: \n","*   X_feats : (|P|, T, D)\n","*   X_times : (|P|, T)\n","*   X_masks : (|P|, T)\n","*   Y_mort : (|P|) \n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"ZPWJ1eybHTIu","colab_type":"code","colab":{}},"source":["X_train_feats = torch.from_numpy(X_train_feats).float()\n","X_train_times = torch.from_numpy(X_train_times).float() \n","X_train_masks = torch.from_numpy(X_train_masks) \n","Y_train_mort = torch.from_numpy(Y_train_mort)\n","Y_train_3d = torch.from_numpy(Y_train_3d)\n","Y_train_7d = torch.from_numpy(Y_train_7d)\n","\n","X_test_feats = torch.from_numpy(X_test_feats).float()\n","X_test_times = torch.from_numpy(X_test_times).float() \n","X_test_masks = torch.from_numpy(X_test_masks) \n","Y_test_mort = torch.from_numpy(Y_test_mort)\n","Y_test_3d = torch.from_numpy(Y_test_3d)\n","Y_test_7d = torch.from_numpy(Y_test_7d)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qyGv5E_HFvuP","colab_type":"text"},"source":["# Run Model"]},{"cell_type":"code","metadata":{"id":"Ax54wDCsL5I1","colab_type":"code","colab":{}},"source":["from torch.utils.data import DataLoader, Dataset\n","\n","BATCH_SIZE = 64\n","\n","class TimeDataset(Dataset): \n","  def __init__(self, xf, xt, xm, ym): \n","    self.src = xf\n","    self.times = xt\n","    self.src_mask = xm\n","    self.trg = ym\n","  def __len__(self): \n","    return self.src.size(0)\n","  def __getitem__(self, index): \n","    return self.src[index], self.times[index], self.src_mask[index], self.trg[index]\n","\n","train_dataset = TimeDataset(X_train_feats, X_train_times, X_train_masks, Y_train_7d)\n","test_dataset = TimeDataset(X_test_feats, X_test_times, X_test_masks, Y_test_7d)\n","train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n","test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"693rNm09TngN","colab_type":"code","colab":{}},"source":["# make_model(d_input, d_target, N=6, M=12, d_model=512, d_hidden=256, d_ff=2048, h=8, max_observation_time=24.0, max_encoding_time=1000.0, dropout=0.1):\n","model = make_model(X_train_feats.size(2), d_target=1, N=4, d_model=128, d_hidden=128, d_ff=512, dropout=0.5, type='CLS')\n","model_opt = get_std_opt(model)\n","model.cpu()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EL-rC9CDj6Zi","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_auc_score\n","from sklearn.metrics import roc_curve\n","\n","def evaluate_result(true_tag_list, predicted_tag_list, probs):\n","  for i in range(len(true_tag_list)): \n","    true_tag_list[i] = true_tag_list[i].item()\n","    predicted_tag_list[i] = predicted_tag_list[i].item()\n","  acc = 1.0 - np.mean(np.abs(np.array(true_tag_list) - np.array(predicted_tag_list)))\n","  return acc, roc_auc_score(true_tag_list, probs)\n","\n","criterion = torch.nn.BCEWithLogitsLoss()\n","criterion.cuda()\n","for epoch in range(40):\n","    train_epoch(train_loader, model, criterion, model_opt)\n","    probs, preds, trgs = valid_epoch(test_loader, model, criterion)\n","    acc, auc = evaluate_result(trgs, preds, probs)\n","    print(\"Epoch \", epoch + 1, \" Acc: \", acc, \" AUC: \", auc)"],"execution_count":0,"outputs":[]}]}